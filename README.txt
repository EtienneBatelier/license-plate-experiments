License place experiments -- 03.2025

This repository contains bits of code experimenting with computer vision concepts. The main objective is modest, and remains to get acquainted with the torch syntax, some image segmentation methods from a practical standpoint. The far-reaching goal of the project is to try and read license plates on pictures of vehicles, using an algorithm built "from the ground up" -- in the sense that we want to avoid the use of large, pre-trained networks. 

Our approach is to specialize and simplify the Region-based convolutional neural network (R-CNN) algorithm introduced by Girshick, Donahue, Darrell and Malik for general object recognition in the 2013 article "Rich feature hierarchies for accurate object detection and semantic segmentation". The R-CNN approach presented in this work has been improved in many ways over time. Without deep conceptual changes to the inner workings, Fast R-CNN and Faster R-CNN have improved the running time (not the training time), getting it from approximately one minute per image down to about one second per image. Nowadays, a different, popular state-of-the-art approach is the YOLO implemented algorithm developped by Ultralytics and made available at https://docs.ultralytics.com/. 

In its original 2013 version, the R-CNN involves three broad steps. The algorithm starts by segmenting an input image to generate proposals for regions of interest. Then, features are extracted from each of these proposals. Finally, the features are used to classify the proposed regions into classes that correspond to the objects that are being detected. The second step -- feature extraction -- involves using a large pre-trained network. This is deemed necessary to get enough features to feed the many classifiers in the step that follows (these classifiers are linear SVMs in the above-mentioned article). The 2013 R-CNN aims to detect objects in a broad sense, with more than 200 classes such as "airplane", "person", "tvmonitor". We merely want to detect characters on license plates, and therefore propose to test if a simplified algorithm would stand a chance. We want to replace the second and third steps with an evaluation by a single, small, convolutional neural network (CNN). To be more precise, we try the following two-step method: first generate proposals for region of interest using an image segmentation algorithm, then feed these proposals to a small, trained CNN to estimate if each region contains a license plate character. The matches are then combined to make a guess for the license plate. The CNN is trained on 35 classes, corresponding to the number from 0 to 9 as well as the letter of the alphabet with the "O" removed (these are the characters one can find on most license plates). 

More precisely, the first segmentation step is identical to that used in the 2013 article above. It is called "selective search" and has been first presented in "Selective Search for Object Recognition" by Uijlings, van de Sande, Gevers and Smeulders in 2012. Selective search works by performing an initial segmentation of an image into small, internally similar regions, then proceeds to merge them into larger regions. The initial segmentation relies on an earlier method of Felzenszwalb and  Huttenlocher originally described in "Efficient Graph-Based Image Segmentation" in 2003. The second step groups regions based on similarity. A portion of Uijlings, van de Sande, Gevers and Smeulders'article is dedicated to the description of various, complementary measures of the similarity between two connected regions of a picture. Our program uses only the most straightforward of these measures: color similarity. The main reason for discarding the rest (which include, among others,  texture similarity) is we believe the color used for the characters of a license plate has strong contrast with the background, but the background and character textures are similar. 

Our Python implementation of Felzenszwalb and Huttenlocher's image segmentation method lies in FH_segmentation.py. It is almost identical to Soumik Rakshit's implementation which he made publicly available on his GitHub page (username soumik12345) at https://github.com/soumik12345/felzenszwalb_segmentation. It is fast and leverages the efficiency of a sleek merge-find set data structure that takes the form of the class DisjointSet. Felzenszwalb and Huttenlocher's method begins by producing a (weighted, undirected) graph out of an image, then uses a graph segmentation procedure. The variable and function names in FH_segmentation.py belong to the graph theory language, and the algorithm could be used in other contexts. Verifying the code provided in FH_segmentation.py really does implement the algorithm presented in Felzenszwalb and Huttenlocher's article is not hard but not immediate. The following key fact about minimum spanning trees of weighted graphs should be kept in mind: whenever a weighted connected graph G is partitioned into two sub-graphs A and B, a minimum spanning trees for G amounts to a pair of minimum spanning trees for A and B, as well as a minimum weight edge connecting A and B. 

The second, hierarchical grouping step can be found in selective_search.py. A segmented picture is again thought of as a (weighted, undirected) graph whose vertex set is a partitioned set. The weighted edges are Python lists [start, end, weight], and the vertex set forms a DisjointSet. This part of the algorithm uses the following two operations extensively: given an edge, access its endpoints; given a vertex, access the edges connected to it. This makes the choice of an efficient data structure for our partitioned graph very hard, and made this part of the algorithm among the slowest ones. A future task could be to come back to this and look for an optimization. The files selective_search.py uses elementary functions manipulating image (via the PIL library) stored in visualization_tools.py. 

The creation and training of the CNN is done is CNN.py. This file contains straightforward torch code. The data is turned into pytorch tensors in lp_data_acquisition.py. We merged to public datasets available on Kaggle at https://www.kaggle.com/datasets/aladdinss/license-plate-digits-classification-dataset/ and https://www.kaggle.com/datasets/preatcher/standard-ocr-dataset. The former consists of rather high quality images of cropped individual license plate characters. Although the Kaggle description does not mention it, it seems like it is already augmented with slight rotations. The latter is a more general optical character recognition dataset. 

Finally, the regions of interest whose evaluations through the CNN suggest they contain a character are combined and filtered by an instance of non-maximum suppression (NMS). The code for this lies in NMS.py. 

The various parts of the code are combined in R-CNN.py. The selective seach part of the algorithm requires three input parameters: k, sigma and min_size. The NMS part requires one, iou_threshold. These parameters were already present in Girshick, Donahue, Darrell and Malik's 2013 approach. The file R_CNN.py contains a function to perform grid-search to try and find optimal values for all four parameters. The test in the grid-search of performed on a set of pictures of cars available at 
https://www.kaggle.com/datasets/andrewmvd/car-plate-detection and https://www.kaggle.com/datasets/abdelhamidzakaria/european-license-plates-dataset/. None of the datasets are included in the git repository, which could lead to execution errors. 







